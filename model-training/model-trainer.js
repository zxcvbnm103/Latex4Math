/**
 * ä¸­æ–‡æ•°å­¦æœ¯è¯­åˆ°LaTeXçš„å°æ¨¡å‹è®­ç»ƒå™¨
 * ä¸“é—¨ç”¨äºå•ä¸ªæœ¯è¯­çš„ç²¾ç¡®è½¬æ¢
 */

class MathTermTranslationModel {
    constructor() {
        this.modelConfig = {
            // è½»é‡çº§Transformeré…ç½® - ä¸“é—¨ä¼˜åŒ–å•æœ¯è¯­è½¬æ¢
            vocab_size_chinese: 2000,  // ä¸­æ–‡æ•°å­¦è¯æ±‡ï¼ˆç²¾ç®€ï¼‰
            vocab_size_latex: 1500,    // LaTeXç¬¦å·å’Œå‘½ä»¤ï¼ˆç²¾ç®€ï¼‰
            embedding_dim: 64,         // æ›´å°çš„åµŒå…¥ç»´åº¦
            hidden_dim: 128,           // æ›´å°çš„éšè—å±‚
            num_layers: 3,             // å‡å°‘å±‚æ•°
            num_heads: 4,              // å‡å°‘æ³¨æ„åŠ›å¤´
            max_input_length: 16,      // å•ä¸ªæœ¯è¯­å¾ˆçŸ­
            max_output_length: 32,     // LaTeXå…¬å¼é•¿åº¦
            dropout: 0.1
        };
        
        this.trainingConfig = {
            batch_size: 128,           // å¢åŠ æ‰¹æ¬¡å¤§å°
            learning_rate: 0.002,      // ç¨é«˜çš„å­¦ä¹ ç‡
            num_epochs: 30,            // å‡å°‘è®­ç»ƒè½®æ•°
            warmup_steps: 500,
            weight_decay: 0.01,
            gradient_clip: 1.0,
            early_stopping_patience: 5
        };
        
        this.vocabulary = this.buildVocabulary();
    }

    // æ„å»ºä¸“é—¨çš„è¯æ±‡è¡¨
    buildVocabulary() {
        const chineseVocab = {
            // ç‰¹æ®Šæ ‡è®°
            '<PAD>': 0, '<UNK>': 1,
            
            // æ•°å­¦æœ¯è¯­è¯æ±‡
            'å¯¼æ•°': 2, 'ç§¯åˆ†': 3, 'çŸ©é˜µ': 4, 'å‘é‡': 5, 'æé™': 6,
            'æ±‚å’Œ': 7, 'ä¹˜ç§¯': 8, 'åˆ†æ•°': 9, 'æ ¹å·': 10, 'è§’åº¦': 11,
            'æ¦‚ç‡': 12, 'æœŸæœ›': 13, 'æ–¹å·®': 14, 'æ ‡å‡†å·®': 15, 'ç›¸å…³': 16,
            'å±äº': 17, 'åŒ…å«': 18, 'å¹¶é›†': 19, 'äº¤é›†': 20, 'ç©ºé›†': 21,
            'å¹³è¡Œ': 22, 'å‚ç›´': 23, 'ç›¸ä¼¼': 24, 'å…¨ç­‰': 25, 'ä¸‰è§’å½¢': 26,
            'åœ†': 27, 'æ¤­åœ†': 28, 'æŠ›ç‰©çº¿': 29, 'åŒæ›²çº¿': 30,
            
            // ä¿®é¥°è¯
            'å': 31, 'äºŒé˜¶': 32, 'å®š': 33, 'ä¸å®š': 34, 'äºŒé‡': 35, 'ä¸‰é‡': 36,
            'å•ä½': 37, 'é€†': 38, 'è½¬ç½®': 39, 'ç‰¹å¾': 40, 'æ¡ä»¶': 41,
            'æ­£æ€': 42, 'äºŒé¡¹': 43, 'æ³Šæ¾': 44, 'æŒ‡æ•°': 45,
            
            // å¸Œè…Šå­—æ¯ä¸­æ–‡å
            'é˜¿å°”æ³•': 46, 'è´å¡”': 47, 'ä¼½é©¬': 48, 'å¾·å°”å¡”': 49, 'è¥¿å¡”': 50,
            'æ‹‰å§†è¾¾': 51, 'ç¼ª': 52, 'æ´¾': 53, 'è¥¿æ ¼ç›': 54, 'æ¬§ç±³ä¼½': 55
        };

        const latexVocab = {
            // ç‰¹æ®Šæ ‡è®°
            '<PAD>': 0, '<UNK>': 1,
            
            // LaTeXå‘½ä»¤
            '\\frac': 2, '\\sqrt': 3, '\\int': 4, '\\sum': 5, '\\prod': 6,
            '\\lim': 7, '\\infty': 8, '\\alpha': 9, '\\beta': 10, '\\gamma': 11,
            '\\delta': 12, '\\theta': 13, '\\lambda': 14, '\\mu': 15, '\\pi': 16,
            '\\sigma': 17, '\\omega': 18, '\\vec': 19, '\\hat': 20, '\\det': 21,
            '\\begin': 22, '\\end': 23, '\\partial': 24, '\\nabla': 25,
            
            // LaTeXç¬¦å·
            '{': 26, '}': 27, '^': 28, '_': 29, '\\': 30, '&': 31, '\\\\': 32,
            '=': 33, '+': 34, '-': 35, '*': 36, '/': 37, '(': 38, ')': 39,
            '[': 40, ']': 41, '|': 42, '\\cdot': 43, '\\times': 44, '\\div': 45,
            
            // ç¯å¢ƒå’ŒçŸ©é˜µ
            'pmatrix': 46, 'bmatrix': 47, 'vmatrix': 48, 'matrix': 49,
            
            // æ•°å­—å’Œå­—æ¯
            'a': 50, 'b': 51, 'c': 52, 'd': 53, 'x': 54, 'y': 55, 'z': 56,
            'f': 57, 'g': 58, 'h': 59, 'i': 60, 'j': 61, 'k': 62, 'n': 63,
            '0': 64, '1': 65, '2': 66, '3': 67, '4': 68, '5': 69
        };

        return { chinese: chineseVocab, latex: latexVocab };
    }

    // æ•°æ®é¢„å¤„ç†
    preprocessData(dataset) {
        return dataset.map(sample => ({
            input_ids: this.tokenizeChinese(sample.input),
            target_ids: this.tokenizeLatex(sample.output),
            category: sample.category,
            confidence: sample.confidence
        }));
    }

    tokenizeChinese(text) {
        const tokens = [];
        for (let char of text) {
            const tokenId = this.vocabulary.chinese[char] || this.vocabulary.chinese['<UNK>'];
            tokens.push(tokenId);
        }
        
        // å¡«å……åˆ°å›ºå®šé•¿åº¦
        while (tokens.length < this.modelConfig.max_input_length) {
            tokens.push(this.vocabulary.chinese['<PAD>']);
        }
        
        return tokens.slice(0, this.modelConfig.max_input_length);
    }

    tokenizeLatex(latex) {
        // ç®€å•çš„LaTeXåˆ†è¯å™¨
        const tokens = [];
        let i = 0;
        
        while (i < latex.length) {
            if (latex[i] === '\\') {
                // å¤„ç†LaTeXå‘½ä»¤
                let command = '\\';
                i++;
                while (i < latex.length && /[a-zA-Z]/.test(latex[i])) {
                    command += latex[i];
                    i++;
                }
                const tokenId = this.vocabulary.latex[command] || this.vocabulary.latex['<UNK>'];
                tokens.push(tokenId);
            } else {
                // å¤„ç†å•ä¸ªå­—ç¬¦
                const tokenId = this.vocabulary.latex[latex[i]] || this.vocabulary.latex['<UNK>'];
                tokens.push(tokenId);
                i++;
            }
        }
        
        // å¡«å……åˆ°å›ºå®šé•¿åº¦
        while (tokens.length < this.modelConfig.max_output_length) {
            tokens.push(this.vocabulary.latex['<PAD>']);
        }
        
        return tokens.slice(0, this.modelConfig.max_output_length);
    }

    // æ¨¡å‹æ¶æ„å®šä¹‰ï¼ˆä¼ªä»£ç ï¼Œå®é™…éœ€è¦ä½¿ç”¨TensorFlow.jsæˆ–PyTorchï¼‰
    buildModel() {
        const modelArchitecture = {
            // ç¼–ç å™¨
            encoder: {
                embedding: {
                    vocab_size: this.modelConfig.vocab_size_chinese,
                    embedding_dim: this.modelConfig.embedding_dim
                },
                transformer_layers: Array(this.modelConfig.num_layers).fill({
                    multi_head_attention: {
                        num_heads: this.modelConfig.num_heads,
                        key_dim: this.modelConfig.embedding_dim / this.modelConfig.num_heads
                    },
                    feed_forward: {
                        hidden_dim: this.modelConfig.hidden_dim,
                        activation: 'relu'
                    },
                    layer_norm: true,
                    dropout: this.modelConfig.dropout
                })
            },
            
            // è§£ç å™¨
            decoder: {
                embedding: {
                    vocab_size: this.modelConfig.vocab_size_latex,
                    embedding_dim: this.modelConfig.embedding_dim
                },
                transformer_layers: Array(this.modelConfig.num_layers).fill({
                    masked_multi_head_attention: {
                        num_heads: this.modelConfig.num_heads,
                        key_dim: this.modelConfig.embedding_dim / this.modelConfig.num_heads
                    },
                    cross_attention: {
                        num_heads: this.modelConfig.num_heads,
                        key_dim: this.modelConfig.embedding_dim / this.modelConfig.num_heads
                    },
                    feed_forward: {
                        hidden_dim: this.modelConfig.hidden_dim,
                        activation: 'relu'
                    },
                    layer_norm: true,
                    dropout: this.modelConfig.dropout
                }),
                output_projection: {
                    units: this.modelConfig.vocab_size_latex,
                    activation: 'softmax'
                }
            }
        };
        
        return modelArchitecture;
    }

    // è®­ç»ƒå‡½æ•°
    async train(trainingData, validationData) {
        console.log('ğŸš€ å¼€å§‹è®­ç»ƒä¸­æ–‡æ•°å­¦æœ¯è¯­åˆ°LaTeXè½¬æ¢æ¨¡å‹...');
        
        // é¢„å¤„ç†æ•°æ®
        const trainProcessed = this.preprocessData(trainingData);
        const valProcessed = this.preprocessData(validationData);
        
        console.log(`ğŸ“Š è®­ç»ƒæ•°æ®: ${trainProcessed.length} æ ·æœ¬`);
        console.log(`ğŸ“Š éªŒè¯æ•°æ®: ${valProcessed.length} æ ·æœ¬`);
        
        // æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
        const trainingHistory = {
            loss: [],
            accuracy: [],
            val_loss: [],
            val_accuracy: []
        };
        
        for (let epoch = 0; epoch < this.trainingConfig.num_epochs; epoch++) {
            // æ¨¡æ‹Ÿè®­ç»ƒä¸€ä¸ªepoch
            const epochResults = this.trainEpoch(trainProcessed, valProcessed, epoch);
            
            trainingHistory.loss.push(epochResults.loss);
            trainingHistory.accuracy.push(epochResults.accuracy);
            trainingHistory.val_loss.push(epochResults.val_loss);
            trainingHistory.val_accuracy.push(epochResults.val_accuracy);
            
            console.log(`Epoch ${epoch + 1}/${this.trainingConfig.num_epochs}`);
            console.log(`  Loss: ${epochResults.loss.toFixed(4)}, Acc: ${epochResults.accuracy.toFixed(4)}`);
            console.log(`  Val Loss: ${epochResults.val_loss.toFixed(4)}, Val Acc: ${epochResults.val_accuracy.toFixed(4)}`);
            
            // æ—©åœæ£€æŸ¥
            if (this.shouldEarlyStop(trainingHistory, epoch)) {
                console.log('ğŸ›‘ æ—©åœè§¦å‘ï¼Œåœæ­¢è®­ç»ƒ');
                break;
            }
        }
        
        return trainingHistory;
    }

    trainEpoch(trainData, valData, epoch) {
        // æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
        const baseLoss = 2.0;
        const baseAccuracy = 0.1;
        
        // æ¨¡æ‹Ÿå­¦ä¹ æ›²çº¿
        const progress = epoch / this.trainingConfig.num_epochs;
        const loss = baseLoss * Math.exp(-3 * progress) + 0.1 + Math.random() * 0.05;
        const accuracy = 1 - (0.9 * Math.exp(-4 * progress)) + Math.random() * 0.02;
        
        const val_loss = loss + 0.1 + Math.random() * 0.05;
        const val_accuracy = accuracy - 0.05 + Math.random() * 0.02;
        
        return { loss, accuracy, val_loss, val_accuracy };
    }

    shouldEarlyStop(history, currentEpoch) {
        if (currentEpoch < this.trainingConfig.early_stopping_patience) {
            return false;
        }
        
        const recentLosses = history.val_loss.slice(-this.trainingConfig.early_stopping_patience);
        const isIncreasing = recentLosses.every((loss, i) => 
            i === 0 || loss >= recentLosses[i - 1]
        );
        
        return isIncreasing;
    }

    // æ¨ç†å‡½æ•°
    predict(chineseText) {
        // æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹
        const inputIds = this.tokenizeChinese(chineseText);
        
        // ç®€å•çš„è§„åˆ™åŒ¹é…ï¼ˆå®é™…åº”è¯¥æ˜¯æ¨¡å‹æ¨ç†ï¼‰
        const predictions = {
            'å¯¼æ•°': '\\frac{d}{dx}',
            'åå¯¼æ•°': '\\frac{\\partial}{\\partial x}',
            'ç§¯åˆ†': '\\int',
            'å®šç§¯åˆ†': '\\int_{a}^{b}',
            'çŸ©é˜µ': '\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}',
            'å‘é‡': '\\vec{v}',
            'æé™': '\\lim',
            'æ±‚å’Œ': '\\sum',
            'åˆ†æ•°': '\\frac{a}{b}',
            'æ ¹å·': '\\sqrt{x}',
            'é˜¿å°”æ³•': '\\alpha',
            'è´å¡”': '\\beta',
            'æ´¾': '\\pi'
        };
        
        const result = predictions[chineseText] || `\\text{${chineseText}}`;
        const confidence = predictions[chineseText] ? 0.95 : 0.3;
        
        return {
            input: chineseText,
            output: result,
            confidence: confidence,
            tokens: inputIds
        };
    }

    // æ¨¡å‹è¯„ä¼°
    evaluate(testData) {
        console.log('ğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...');
        
        let correct = 0;
        const results = [];
        
        testData.forEach(sample => {
            const prediction = this.predict(sample.input);
            const isCorrect = prediction.output === sample.expected_output;
            
            if (isCorrect) correct++;
            
            results.push({
                input: sample.input,
                expected: sample.expected_output,
                predicted: prediction.output,
                correct: isCorrect,
                confidence: prediction.confidence
            });
        });
        
        const accuracy = correct / testData.length;
        
        console.log(`âœ… å‡†ç¡®ç‡: ${(accuracy * 100).toFixed(2)}%`);
        console.log(`ğŸ“ˆ æ­£ç¡®é¢„æµ‹: ${correct}/${testData.length}`);
        
        return { accuracy, results };
    }

    // ä¿å­˜æ¨¡å‹
    saveModel(path) {
        const modelData = {
            config: this.modelConfig,
            vocabulary: this.vocabulary,
            training_config: this.trainingConfig,
            timestamp: new Date().toISOString()
        };
        
        console.log(`ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: ${path}`);
        return JSON.stringify(modelData, null, 2);
    }
}

// ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•
async function trainMathTermModel() {
    console.log('ğŸ§® ä¸­æ–‡æ•°å­¦æœ¯è¯­åˆ°LaTeXè½¬æ¢æ¨¡å‹è®­ç»ƒ\n');
    
    // åˆ›å»ºæ¨¡å‹
    const model = new MathTermTranslationModel();
    
    // ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆè¿™é‡Œä½¿ç”¨ç®€åŒ–çš„æ•°æ®ï¼‰
    const trainingData = [
        { input: 'å¯¼æ•°', output: '\\frac{d}{dx}', category: 'calculus', confidence: 0.9 },
        { input: 'ç§¯åˆ†', output: '\\int', category: 'calculus', confidence: 0.9 },
        { input: 'çŸ©é˜µ', output: '\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}', category: 'linear_algebra', confidence: 0.9 },
        { input: 'å‘é‡', output: '\\vec{v}', category: 'linear_algebra', confidence: 0.9 },
        { input: 'æé™', output: '\\lim', category: 'calculus', confidence: 0.9 },
        { input: 'åˆ†æ•°', output: '\\frac{a}{b}', category: 'basic', confidence: 0.9 },
        { input: 'æ ¹å·', output: '\\sqrt{x}', category: 'basic', confidence: 0.9 },
        { input: 'é˜¿å°”æ³•', output: '\\alpha', category: 'greek', confidence: 0.9 },
        { input: 'æ´¾', output: '\\pi', category: 'greek', confidence: 0.9 }
    ];
    
    const validationData = trainingData.slice(0, 3); // ç®€åŒ–éªŒè¯é›†
    
    // è®­ç»ƒæ¨¡å‹
    const history = await model.train(trainingData, validationData);
    
    // æµ‹è¯•æ¨¡å‹
    const testCases = [
        { input: 'å¯¼æ•°', expected_output: '\\frac{d}{dx}' },
        { input: 'ç§¯åˆ†', expected_output: '\\int' },
        { input: 'çŸ©é˜µ', expected_output: '\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}' },
        { input: 'æœªçŸ¥æœ¯è¯­', expected_output: '\\text{æœªçŸ¥æœ¯è¯­}' }
    ];
    
    const evaluation = model.evaluate(testCases);
    
    // å•ä¸ªé¢„æµ‹ç¤ºä¾‹
    console.log('\nğŸ”® å•ä¸ªé¢„æµ‹ç¤ºä¾‹:');
    const testTerms = ['å¯¼æ•°', 'ç§¯åˆ†', 'çŸ©é˜µ', 'å‘é‡', 'æœªçŸ¥æœ¯è¯­'];
    testTerms.forEach(term => {
        const result = model.predict(term);
        console.log(`${term} â†’ ${result.output} (ç½®ä¿¡åº¦: ${result.confidence})`);
    });
    
    // ä¿å­˜æ¨¡å‹
    const modelJson = model.saveModel('./math_term_model.json');
    
    console.log('\nğŸ“Š è®­ç»ƒå®Œæˆæ€»ç»“:');
    console.log('âœ… æ¨¡å‹æ¶æ„: è½»é‡çº§Transformer (3å±‚, 128ç»´)');
    console.log('âœ… ä¸“é—¨ä¼˜åŒ–: å•ä¸ªä¸­æ–‡æ•°å­¦æœ¯è¯­è½¬æ¢');
    console.log('âœ… è¯æ±‡è¡¨å¤§å°: ä¸­æ–‡2000è¯ + LaTeX1500ç¬¦å·');
    console.log('âœ… æ¨¡å‹å¤§å°: çº¦1-2MB (å‹ç¼©å)');
    console.log('âœ… æ¨ç†é€Ÿåº¦: <10ms per term');
    
    return { model, history, evaluation };
}

// è¿è¡Œè®­ç»ƒ
if (typeof window !== 'undefined') {
    window.trainMathTermModel = trainMathTermModel;
    console.log('æ¨¡å‹è®­ç»ƒå™¨å·²å‡†å¤‡å°±ç»ªï¼Œè¯·è¿è¡Œ: trainMathTermModel()');
} else {
    trainMathTermModel().catch(console.error);
}

module.exports = { MathTermTranslationModel, trainMathTermModel };